vit:
  emb_dropout: 0.0
  patch_length: 197
  patch_size: 16
  num_layers: 24
  num_heads: 48
  embed_dim: 3072
  mlp_dim: 12288
  use_native_attention: False
partial_pretrain: []
bert:
  hidden_size: 3072
  intermediate_size: 12288
  num_hidden_layers: 24
  num_attention_heads: 48