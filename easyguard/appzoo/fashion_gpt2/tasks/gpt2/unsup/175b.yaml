network:
  hidden_size: 12288
  n_embed: 12288  # if vocab embedding == hidden_size, then no extra projection
  n_inner: 49152
  n_head: 96
  n_layer: 96
  vocab_size: 145664
  max_position_embeddings: 2048
  layer_norm_epsilon: 1.0e-5
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0.1
  attn_pdrop: 0.1
  scale_attn_weights: true # TODO:
  scale_attn_by_inverse_layer_idx: false # TODO:
  reorder_and_upcast_attn: false # TODO:
  initializer_range: 0.00570544330734548
  gradient_checkpointing: false
  tie_weight: true
