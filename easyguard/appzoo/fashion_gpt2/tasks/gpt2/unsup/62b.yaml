network:
  hidden_size: 8192
  n_embed: 4096  # vocab embedding, low-rank
  n_inner: 32768
  n_head: 64
  n_layer: 64
  vocab_size: 145664
  max_position_embeddings: 2048
  layer_norm_epsilon: 1.0e-5
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0
  attn_pdrop: 0.1
  scale_attn_weights: true # TODO:
  scale_attn_by_inverse_layer_idx: false # TODO:
  reorder_and_upcast_attn: false # TODO:
  initializer_range: 0.0006176323555016366
  gradient_checkpointing: false
  tie_weight: true

#Slightly modified from OPT and PaLM