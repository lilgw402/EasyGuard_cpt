ACCELERATOR:
  CLIP_GRAD_NORM: 10.0
  FP16: true
  FP16_LOSS_SCALE: dynamic
  FP16_OPT_LEVEL: O2
  GRAD_ACCUMULATE_STEPS: 8
  SYNCBN: false


BERT:
  layernorm_eps: 1.0e-6
  act_dropout_prob: 0.0
  attention_probs_dropout_prob: 0.1
  embedding_process_config:
    dropout_prob: 0.1
    epsilon: 1.0e-06
    norm_type: layer
    sequence: nd
  embedding_size: 256
  frozen_layers: -1
  hidden_act: gelu
  hidden_dropout_prob: 0.1
  hidden_size: 768
  initializer_config:
    mean: 0.0
    name: truncated_normal
    stddev: 0.02
  initializer_range: 0.02
  intermediate_size: 3072
  max_position_embeddings: 512
  model_name: ALBertModel
  num_attention_heads: 12
  num_hidden_layers: 6
  post_layer_process_config:
    dropout_prob: 0.1
    epsilon: 1.0e-06
    norm_type: layer
    sequence: dan
  pre_layer_process_config:
    dropout_prob: 0.1
    epsilon: 1.0e-06
    norm_type: layer
    sequence: none
  project_embedding_first: true
  type_vocab_size: 16
  use_bias: true
  use_fp16: true
  vocab_size: 145608
  with_pooler: true
  word_embedding_frozen: false


DATASET:
  ALREADY_PREPROCESS: false
  DATASET: TusouDataset
  FIELDS:
  - cut_title
  IMAGE_MAX_SIZE: 256
  IMAGE_MIN_SIZE: 256
  MASK_STYLE: ernie
  NEED_QUERY: false
  NEED_TRANSPOSE: false
  PIXEL_MEANS:
  - 0.485
  - 0.456
  - 0.406
  PIXEL_STDS:
  - 0.229
  - 0.224
  - 0.225
  SEQ_LEN: 32
  TOBGR255: false
  TRAIN_PATH: hdfs://haruna/home/byte_search_nlp_lq/user/huangwenguan/data/tusou/tusou_14b_cut_rn_flt_trn_r008
  VAL_PATH: hdfs://haruna/home/byte_search_nlp_lq/user/huangwenguan/data/tusou/tusou_14b_cut_rn_flt_val_r
NETWORK:
  FREEZE_BN_RUNNING_STATS: false
  FREEZE_RESNET_LAYERS:
  - 1
  - 2
  - 3
  - 4
  - 5
  - 6
  NET: VMLMNet
  PARTIAL_PRETRAIN:
  - hdfs://haruna/home/byte_search_nlp_lq/user/huangwenguan/modelhub/albert_6l_zh_mix_oldcut_20200921/fex/model_t_fp32_ln.th
  - hdfs://haruna/home/byte_search_nlp_lq/user/huangwenguan/clue/img_dali_616_b256_e32/model_state_epoch_56249.th
  PARTIAL_PRETRAIN_PREFIX_CHANGES:
  - token_embedder_tokens->not_used.token_embedder_tokens
  - t_proj->not_used.t_proj
  - resnet->albertv.visual_tokenizer.resnet
  - v_proj->albertv.visual_tokenizer.v_proj
  - v_layer_norm->albertv.visual_tokenizer.v_layer_norm
  - pooler->nouse.pooler
  - ->albertv.
  VOCAB_FILE: hdfs://haruna/home/byte_search_nlp_lq/user/huangwenguan/modelhub/albert_6l_zh_mix_oldcut_20200921/archer/zh_old_cut_145607.vocab
  W2V_PATH: hdfs://haruna/home/byte_search_nlp_lq/user/huangwenguan/modelhub/albert_6l_zh_mix_oldcut_20200921/fex/embedding_proj.th
  WITH_MLM_LOSS: true
  WITH_REL_LOSS: true
  activ_and_nobias: true
  embedding_size: 256
  loss_type: cosine
  v_norm: true


TRAIN:
  LR: 0.0001
  LR_SCHEDULE: triangle
  NUM_WORKERS: 32
  OPTIMIZER: AdamW
  SHUFFLE: true
  WARMUP: true
  WARMUP_FACTOR: 0.1
  WARMUP_METHOD: linear
  WARMUP_STEPS: 10000
  WD: 0.0001


TRAINER:
  BEGIN_EPOCH: 0
  CHANNELS_LAST: false
  CHECKPOINT_FREQUENT: 16000
  END_EPOCH: 5
  LOG_FREQUENT: 100
  RNG_SEED: 42
  TRAIN_BATCH_SIZE: 384
  TRAIN_DATASET_SIZE: 107802798
  VAL_BATCH_SIZE: 16
  VAL_DATASET_SIZE: 1119030
  VAL_FREQUENT: 5000
  VAL_STEPS: 10

  
VAL:
  NUM_WORKERS: 4
