deepspeed: valley/configs/deepspeed/config_zero2.json
model_class: valley-product
model_name_or_path: /mnt/bn/valley2/hezhongheng/continue_pretrain_multi/valley_ckpt
data_path: /mnt/bn/valley2/hezhongheng/continue_pretrain_multi/data/train_data/v1_data/mllm_sft_data.txt
lora_model: /mnt/bn/valley2/hezhongheng/continue_pretrain_multi/ckpt/v_2_2/sft_p_cot_post2_ckpt/checkpoint-6800
image_folder: /mnt/bn/valley2/hezhongheng/mllm_image/continut_pretrain
project_name: product_continue_pretrian
run_name: valley-pool_adapter-continue_pretrain-down-lora-v1-2
# english or chinese, default is english
language: chinese
vision_tower: /mnt/bn/valley2/hezhongheng/continue_pretrain_multi/vison_tower
version: "v0"
prompt_version: "conv_prd_cp"
only_mask_system: False
mm_vision_select_feature: 'cls_patch'
mm_vision_select_layer: -2
mm_use_im_start_end: True
mm_use_im_patch_token: False
mm_projector_type: pool_adapter
image_crop_width: 448
image_crop_height: 448
pool_out_size: 8
max_img_num: 8
tune_mm_mlp_adapter: True
freeze_backbone: True
group_by_modality_length: True
bf16: False
fp16: True
lora_enable: True
output_dir: /mnt/bn/valley2/hezhongheng/continue_pretrain_multi/ckpt/v_2_1/sft_from_p_cotv3_ckpt
num_train_epochs: 5
per_device_train_batch_size: 1
per_device_eval_batch_size: 1
gradient_accumulation_steps: 1
evaluation_strategy: "no"
save_strategy: "no"
lora_save_strategy: "steps"
save_steps: 1562
learning_rate: 0.0001
weight_decay: 0.
warmup_ratio: 0.00
lr_scheduler_type: cosine
logging_steps: 1
tf32: False
model_max_length: 2048
gradient_checkpointing: True
dataloader_num_workers: 4
lazy_preprocess: True
report_to: wandb