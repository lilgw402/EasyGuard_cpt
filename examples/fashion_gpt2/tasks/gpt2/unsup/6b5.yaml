network:
  hidden_size: 4096
  n_embed: 2048  # vocab embedding
  n_inner: 16384
  n_head: 32
  n_layer: 32
  vocab_size: 145664
  max_position_embeddings: 2048
  layer_norm_epsilon: 1.0e-5
  activation_function: gelu_new
  resid_pdrop: 0.1
  embd_pdrop: 0
  attn_pdrop: 0.1
  scale_attn_weights: true # TODO:
  scale_attn_by_inverse_layer_idx: false # TODO:
  reorder_and_upcast_attn: false # TODO:
  initializer_range: 0.009882117688026186
  gradient_checkpointing: false
  tie_weight: true

# Ref: https://arxiv.org/abs/2005.14165 Table 2.1