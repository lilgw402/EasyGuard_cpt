================= Ê¨¢Ëøé‰ΩøÁî®MLX Lab =================
MLX LabÂºÄÂèëÊú∫‰ΩøÁî®CPUÊú∫Âô®‰Ωú‰∏∫MasterËäÇÁÇπÔºåÂ¶ÇÈúÄ‰ΩøÁî®GPUË∞ÉËØïÔºåËØ∑‰ΩøÁî®MLX CLIÂëΩ‰ª§Ë°åÂ∑•ÂÖ∑launchËøúÁ´ØGPU workerËäÇÁÇπ„ÄÇ
ÂêåÊó∂ÔºåÊÇ®‰πüÂèØ‰ΩøÁî®MLX CLI Êèê‰∫§Ê≠£ÂºèËÆ≠ÁªÉÔºåËØ¶ÊÉÖËØ∑ÂèÇËÄÉ‰ª•‰∏ãÊñáÊ°£:
MLX CLI‰ΩøÁî®ÊñáÊ°£Ôºöhttps://bytedance.feishu.cn/wiki/wikcnLdUEl6ldPC74uhCwnJZEHz
MLX LabÁî®Êà∑ÊåáÂçóÔºöhttps://bytedance.feishu.cn/wiki/wikcn0DQhxqPbF34xW8bI1csEnd
===================================================
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# nvis[K[K[K[Knvidia-smi 
Fri Mar  3 15:06:59 2023       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 450.191.01   Driver Version: 450.191.01   CUDA Version: 11.3     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  Tesla V100-SXM2...  On   | 00000000:1B:00.0 Off |                    0 |
| N/A   45C    P0    77W / 300W |      0MiB / 32510MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# nvidia-smi head /mnt/bn/multimodel-pretrain/benchmark/live_ecology_label_data/site_new/meta/train_metas.json[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cll[Khdfs dfs -get hdfs://haruna/home/byte_ecom_govern/user/yangmin.priv/1e_fvtp3_continue_pretrain_0220_8new/rank0[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[1Pls[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C^C
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# yto[K[K[Kpyton[K[K[K[K[Kpython3 examples/fashion[K[K[K[K[K[K[Kimage_classification/run_model_finetuning.py --config ./[K[Kexamples/fi[K[Kia[Kmage_classification/
config_optim.yaml        data.py                  default_config_old.yaml  feature_extractor.py     ptms.png                 readme.md                
data/                    debug_data.py            default_config.yaml      .ipynb_checkpoints/      __pycache__/             run_model_finetuning.py  
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# python3 examples/image_classification/run_model_finetuning.py --config examples/image_classification/
config_optim.yaml        data.py                  default_config_old.yaml  feature_extractor.py     ptms.png                 readme.md                
data/                    debug_data.py            default_config.yaml      .ipynb_checkpoints/      __pycache__/             run_model_finetuning.py  
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# python3 examples/image_classification/run_model_finetuning.py --config examples/image_classification/
config_optim.yaml        data.py                  default_config_old.yaml  feature_extractor.py     ptms.png                 readme.md                
data/                    debug_data.py            default_config.yaml      .ipynb_checkpoints/      __pycache__/             run_model_finetuning.py  
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# python3 examples/image_classification/run_model_finetuning.py --config examples/image_classification/run_model_finetuning.py [K[K[K[K[K[K[K[K[K[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K[K[K[K[K[K[K[K[K[K[K[K[Kdefault_config [A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kg
^CTraceback (most recent call last):
  File "examples/image_classification/run_model_finetuning.py", line 17, in <module>
    import torch
  File "/usr/local/lib/python3.7/dist-packages/torch/__init__.py", line 58, in <module>
    init_byted_base()
  File "/usr/local/lib/python3.7/dist-packages/torch/_byted_config.py", line 127, in init_byted_base
    extract_lego_ops_info()
  File "/usr/local/lib/python3.7/dist-packages/torch/_byted_config.py", line 115, in extract_lego_ops_info
    cudnn_result = subprocess.run(cudnn_cmd, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
  File "/usr/lib/python3.7/subprocess.py", line 474, in run
    stdout, stderr = process.communicate(input, timeout=timeout)
  File "/usr/lib/python3.7/subprocess.py", line 939, in communicate
    stdout, stderr = self._communicate(input, endtime, timeout)
  File "/usr/lib/python3.7/subprocess.py", line 1681, in _communicate
    ready = selector.select(timeout)
  File "/usr/lib/python3.7/selectors.py", line 415, in select
    fd_event_list = self._selector.poll(timeout)
KeyboardInterrupt
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# python3 examples/image_classification/run_model_finetuning.py --config examples/image_classification/default_config [A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[K
[K[A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Cg .yaml 
2023-03-03 15:07:56,779 - easyguard.utils.import_utils - INFO - PyTorch version 1.10.0 available.
2023-03-03 15:07:58,564 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]

***********
{'config': None,
 'data': Namespace(data_path='/mnt/bn/multimodel-pretrain/database/', num_workers=8, train_batch_size=128, train_split='/mnt/bn/multimodel-pretrain/database/train_list.txt', val_batch_size=64, val_split='/mnt/bn/multimodel-pretrain/database/val_list.txt'),
 'log_level': 'INFO',
 'model': Namespace(config_optim='./examples/image_classification/config_optim.yaml', model_arch='fashion-swin-base-224-fashionvtp'),
 'trainer': Namespace(accelerator='gpu', accelerator_kwargs={}, accumulate_grad_batches=None, benchmark=False, callbacks=None, checkpoint_mode='max', checkpoint_monitor='val_acc', dataloader_retry_limit=100, dataloader_retry_persistent_limit=5, dataloader_timeout=-1, default_hdfs_dir=None, default_root_dir=None, detect_anomaly=False, deterministic=False, enable_checkpoint=True, enable_qat=False, enable_speedmonitor=True, enable_trace=False, enable_versions=True, experiment_name=None, find_unused_parameters=True, grad_norm_layers=[], gradient_clip_val=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=100, max_steps=-1, optimizer_kwargs={'optimizer': {'type': 'torch.optim.AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.01, 'correct_bias': True, 'correct_bias_eps': False, 'bias_correction': True, 'adam_w_mode': True, 'amsgrad': False, 'set_grad_none': True, 'momentum': 0.0, 'nesterov': False}}, 'scheduler': {'type': 'cruise.optim.constant_lr', 'total_steps_param_name': 'total_iters', 'warmup_steps_param_name': 'warmup_iters', 'interval': 'epoch_end', 'params': {'warmup_step_rate': 0.0, 'start_factor': 0.3333333333333333, 'end_factor': 1e-07, 'num_cycles': 0.5, 'lr_end': 1e-07, 'power': 1.0}}}, precision=32, project_name=None, qat_kwargs={}, reload_dataloaders_every_n_epochs=-1, resume_ckpt_path=None, resume_loader_state=False, seed=None, strategy='ddp', summarize_model_depth=2, sync_batchnorm=False, sync_fit_metrics=None, val_check_interval=[1000, 1.0])}
***********

2023-03-03 15:07:58,564 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Adjusting root logger level to INFO

2023-03-03 15:07:58,685 INFO MainProcess set_nvidia_flags[/usr/local/lib/python3.7/dist-packages/cruise/trainer/accelerator/gpu.py:70]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

2023-03-03 15:07:58,685 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup validation every 1000 iteration steps

2023-03-03 15:07:58,685 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup validation every 1.0 epoch

2023-03-03 15:07:58,703 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Using version control root dir: "/mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_4", you can turn off this behavior by `enable_versions=false`

2023-03-03 15:07:58,704 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup ModelCheckpoint with local topk=10, hdfs topk=10

2023-03-03 15:07:58,756 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Full CLI saved to: /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_4/cruise_cli.yaml, /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_4/cruise_cli.json

2023-03-03 15:08:04,564 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:159]
start to download `hdfs://haruna/home/byte_ecom_govern/easyguard/models/fashion_swin_base_224_fashionvtp/config.yaml` to local path `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a`

2023-03-03 15:08:18,935 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:159]
start to download `hdfs://haruna/home/byte_ecom_govern/easyguard/models/fashion_swin_base_224_fashionvtp/pytorch_model.pth` to local path `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a`

2023-03-03 15:09:33,745 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:127]
obtain the local file `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a/config.yaml`

/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-03-03 15:09:35,706 INFO MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:497]
Pretrained: no missing_keys.

2023-03-03 15:09:35,707 WARNING MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:503]
=> Pretrained: unexpected_keys: 
+-----------------+
| unexpected_keys |
+-----------------+
| head.bias       |
| head.weight     |
+-----------------+


2023-03-03 15:09:35,707 INFO MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:508]
=> Pretrained: load pretrained model with strict=False

[34m[1mwandb[0m: ‚≠êÔ∏è View project at [34m[4mhttps://ml.bytedance.net/experiment/tracking/detail?Id=project_20230116_29c3c128[0m
[34m[1mwandb[0m: üöÄ View run at [34m[4mhttps://ml.bytedance.net/experiment/tracking/detail?Id=project_20230116_29c3c128&selectedTrial=run_20230303_89b6c679[0m
[34m[1mwandb[0m: wandb version 0.13.10 is available!  To upgrade, please run:
[34m[1mwandb[0m:  $ pip install wandb --upgrade
[34m[1mwandb[0m: Tracking run with wandb version 0.13.7
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/mnt/bn/multimodel-pretrain/codes/EasyGuard/wandb/run-20230303_150957-2pif1dld[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
2023-03-03 15:09:59,830 INFO MainProcess _summarize_model[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2240]

  | Name              | Type              | Params
--------------------------------------------------------
0 | model             | FashionSwinModel  | 87.8 M
1 | model.patch_embed | PatchEmbed        | 6.5 K 
2 | model.pos_drop    | Dropout           | 0     
3 | model.layers      | ModuleList        | 86.7 M
4 | model.norm        | LayerNorm         | 2.0 K 
5 | model.avgpool     | AdaptiveAvgPool1d | 0     
6 | model.head        | Linear            | 1.0 M 
--------------------------------------------------------
87.8 M    Trainable params
0         Non-trainable params
87.8 M    Total params
351.073   Total estimated model params size (MB)

2023-03-03 15:09:59,831 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
===<PROGRESS: 0.00%, Epoch 0/100, 234 steps per epoch>===

2023-03-03 15:10:03,132 ERROR MainProcess _call_and_handle_interrupt_context[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:1120]
[Rank 0] teardown due to exception:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1103, in _call_and_handle_interrupt_context
    yield
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 708, in fit
    self._train_one_epoch()
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1280, in _train_one_epoch
    output_dict, losses = self._train_one_step_forward(batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1134, in _train_one_step_forward
    output_dict = self.model(device_batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/cruise/module/wrapper.py", line 28, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "examples/image_classification/run_model_finetuning.py", line 55, in training_step
    x, y = batch['image']['data'], batch['label']
TypeError: list indices must be integers or slices, not str


2023-03-03 15:10:05,079 INFO MainProcess _log_info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2262]
[Rank 0] CruiseTrainer.save_checkpoint: Checkpoint saved to /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_4/checkpoints/epoch=0-step=0_interupted.ckpt

Traceback (most recent call last):
  File "examples/image_classification/run_model_finetuning.py", line 139, in <module>
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 708, in fit
    self._train_one_epoch()
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1280, in _train_one_epoch
    output_dict, losses = self._train_one_step_forward(batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1134, in _train_one_step_forward
    output_dict = self.model(device_batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/cruise/module/wrapper.py", line 28, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "examples/image_classification/run_model_finetuning.py", line 55, in training_step
    x, y = batch['image']['data'], batch['label']
TypeError: list indices must be integers or slices, not str
[34m[1mwandb[0m: Waiting for W&B process to finish... [31m(failed 1).[0m Press Control-C to abort syncing.
[34m[1mwandb[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[34m[1mwandb[0m: Find logs at: [35m[1m./wandb/run-20230303_150957-2pif1dld/logs[0m
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# python3 examples/image_classification/run_model_finetuning.py --config examples/image_classification/default_config.yaml 
2023-03-03 15:16:31,682 - easyguard.utils.import_utils - INFO - PyTorch version 1.10.0 available.
2023-03-03 15:16:32,748 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]

***********
{'config': None,
 'data': Namespace(data_path='/mnt/bn/multimodel-pretrain/database/', num_workers=8, train_batch_size=128, train_split='/mnt/bn/multimodel-pretrain/database/train_list.txt', val_batch_size=64, val_split='/mnt/bn/multimodel-pretrain/database/val_list.txt'),
 'log_level': 'INFO',
 'model': Namespace(config_optim='./examples/image_classification/config_optim.yaml', model_arch='fashion-swin-base-224-fashionvtp'),
 'trainer': Namespace(accelerator='gpu', accelerator_kwargs={}, accumulate_grad_batches=None, benchmark=False, callbacks=None, checkpoint_mode='max', checkpoint_monitor='val_acc', dataloader_retry_limit=100, dataloader_retry_persistent_limit=5, dataloader_timeout=-1, default_hdfs_dir=None, default_root_dir=None, detect_anomaly=False, deterministic=False, enable_checkpoint=True, enable_qat=False, enable_speedmonitor=True, enable_trace=False, enable_versions=True, experiment_name=None, find_unused_parameters=True, grad_norm_layers=[], gradient_clip_val=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=100, max_steps=-1, optimizer_kwargs={'optimizer': {'type': 'torch.optim.AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.01, 'correct_bias': True, 'correct_bias_eps': False, 'bias_correction': True, 'adam_w_mode': True, 'amsgrad': False, 'set_grad_none': True, 'momentum': 0.0, 'nesterov': False}}, 'scheduler': {'type': 'cruise.optim.constant_lr', 'total_steps_param_name': 'total_iters', 'warmup_steps_param_name': 'warmup_iters', 'interval': 'epoch_end', 'params': {'warmup_step_rate': 0.0, 'start_factor': 0.3333333333333333, 'end_factor': 1e-07, 'num_cycles': 0.5, 'lr_end': 1e-07, 'power': 1.0}}}, precision=32, project_name=None, qat_kwargs={}, reload_dataloaders_every_n_epochs=-1, resume_ckpt_path=None, resume_loader_state=False, seed=None, strategy='ddp', summarize_model_depth=2, sync_batchnorm=False, sync_fit_metrics=None, val_check_interval=[1000, 1.0])}
***********

2023-03-03 15:16:32,748 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Adjusting root logger level to INFO

2023-03-03 15:16:32,801 INFO MainProcess set_nvidia_flags[/usr/local/lib/python3.7/dist-packages/cruise/trainer/accelerator/gpu.py:70]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

2023-03-03 15:16:32,801 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup validation every 1000 iteration steps

2023-03-03 15:16:32,801 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup validation every 1.0 epoch

2023-03-03 15:16:32,808 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Using version control root dir: "/mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_5", you can turn off this behavior by `enable_versions=false`

2023-03-03 15:16:32,808 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup ModelCheckpoint with local topk=10, hdfs topk=10

2023-03-03 15:16:32,845 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Full CLI saved to: /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_5/cruise_cli.yaml, /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_5/cruise_cli.json

2023-03-03 15:16:32,854 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:127]
obtain the local file `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a/config.yaml`

2023-03-03 15:16:32,855 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:127]
obtain the local file `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a/pytorch_model.pth`

/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-03-03 15:16:34,827 INFO MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:497]
Pretrained: no missing_keys.

2023-03-03 15:16:34,828 WARNING MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:503]
=> Pretrained: unexpected_keys: 
+-----------------+
| unexpected_keys |
+-----------------+
| head.bias       |
| head.weight     |
+-----------------+


2023-03-03 15:16:34,828 INFO MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:508]
=> Pretrained: load pretrained model with strict=False

[34m[1mwandb[0m: ‚≠êÔ∏è View project at [34m[4mhttps://ml.bytedance.net/experiment/tracking/detail?Id=project_20230116_29c3c128[0m
[34m[1mwandb[0m: üöÄ View run at [34m[4mhttps://ml.bytedance.net/experiment/tracking/detail?Id=project_20230116_29c3c128&selectedTrial=run_20230303_520978a4[0m
[34m[1mwandb[0m: wandb version 0.13.10 is available!  To upgrade, please run:
[34m[1mwandb[0m:  $ pip install wandb --upgrade
[34m[1mwandb[0m: Tracking run with wandb version 0.13.7
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/mnt/bn/multimodel-pretrain/codes/EasyGuard/wandb/run-20230303_151644-xtwlirtk[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
2023-03-03 15:16:46,924 INFO MainProcess _summarize_model[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2240]

  | Name              | Type              | Params
--------------------------------------------------------
0 | model             | FashionSwinModel  | 86.9 M
1 | model.patch_embed | PatchEmbed        | 6.5 K 
2 | model.pos_drop    | Dropout           | 0     
3 | model.layers      | ModuleList        | 86.7 M
4 | model.norm        | LayerNorm         | 2.0 K 
5 | model.avgpool     | AdaptiveAvgPool1d | 0     
6 | model.head        | Linear            | 205 K 
--------------------------------------------------------
86.9 M    Trainable params
0         Non-trainable params
86.9 M    Total params
347.793   Total estimated model params size (MB)

2023-03-03 15:16:46,925 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
===<PROGRESS: 0.00%, Epoch 0/100, 234 steps per epoch>===

2023-03-03 15:16:49,980 ERROR MainProcess _call_and_handle_interrupt_context[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:1120]
[Rank 0] teardown due to exception:
Traceback (most recent call last):
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1103, in _call_and_handle_interrupt_context
    yield
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 708, in fit
    self._train_one_epoch()
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1280, in _train_one_epoch
    output_dict, losses = self._train_one_step_forward(batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1134, in _train_one_step_forward
    output_dict = self.model(device_batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/cruise/module/wrapper.py", line 28, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "examples/image_classification/run_model_finetuning.py", line 57, in training_step
    y_hat = self(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "examples/image_classification/run_model_finetuning.py", line 51, in forward
    x = self.model(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 877, in forward
    out = self.forward_features(x)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 869, in forward_features
    x = layer(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 656, in forward
    x = blk(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 551, in forward
    x = x + self.drop_path(self.mlp(self.norm2(x)))  # original, pre-LN
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 366, in forward
    x = self.act(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py", line 652, in forward
    return F.gelu(input)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 31.75 GiB total capacity; 29.64 GiB already allocated; 85.75 MiB free; 29.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF


2023-03-03 15:16:51,893 INFO MainProcess _log_info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2262]
[Rank 0] CruiseTrainer.save_checkpoint: Checkpoint saved to /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_5/checkpoints/epoch=0-step=0_interupted.ckpt

Traceback (most recent call last):
  File "examples/image_classification/run_model_finetuning.py", line 141, in <module>
    trainer.fit(model, datamodule)
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 708, in fit
    self._train_one_epoch()
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1280, in _train_one_epoch
    output_dict, losses = self._train_one_step_forward(batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py", line 1134, in _train_one_step_forward
    output_dict = self.model(device_batch, batch_idx)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/cruise/module/wrapper.py", line 28, in forward
    output = self.module.training_step(*inputs, **kwargs)
  File "examples/image_classification/run_model_finetuning.py", line 57, in training_step
    y_hat = self(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "examples/image_classification/run_model_finetuning.py", line 51, in forward
    x = self.model(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 877, in forward
    out = self.forward_features(x)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 869, in forward_features
    x = layer(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 656, in forward
    x = blk(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 551, in forward
    x = x + self.drop_path(self.mlp(self.norm2(x)))  # original, pre-LN
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/modelzoo/models/fashion_swin/modeling_fashion_swin.py", line 366, in forward
    x = self.act(x)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py", line 1118, in _call_impl
    return forward_call(*input, **kwargs)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/activation.py", line 652, in forward
    return F.gelu(input)
  File "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py", line 1556, in gelu
    return torch._C._nn.gelu(input)
RuntimeError: CUDA out of memory. Tried to allocate 98.00 MiB (GPU 0; 31.75 GiB total capacity; 29.64 GiB already allocated; 85.75 MiB free; 29.71 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
[34m[1mwandb[0m: Waiting for W&B process to finish... [31m(failed 1).[0m Press Control-C to abort syncing.





[34m[1mwandb[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)
[34m[1mwandb[0m: Find logs at: [35m[1m./wandb/run-20230303_151644-xtwlirtk/logs[0m


mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# 
mlxlabmrvtguex63e4f79a-20230209133939-yagkks-co5rsd-worker(@:):EasyGuard# python3 examples/image_classification/run_model_finetuning.py --config examples/image_classification/default_config.yaml 
2023-03-03 15:17:41,997 - easyguard.utils.import_utils - INFO - PyTorch version 1.10.0 available.
2023-03-03 15:17:43,058 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]

***********
{'config': None,
 'data': Namespace(data_path='/mnt/bn/multimodel-pretrain/database/', num_workers=8, train_batch_size=64, train_split='/mnt/bn/multimodel-pretrain/database/train_list.txt', val_batch_size=64, val_split='/mnt/bn/multimodel-pretrain/database/val_list.txt'),
 'log_level': 'INFO',
 'model': Namespace(config_optim='./examples/image_classification/config_optim.yaml', model_arch='fashion-swin-base-224-fashionvtp'),
 'trainer': Namespace(accelerator='gpu', accelerator_kwargs={}, accumulate_grad_batches=None, benchmark=False, callbacks=None, checkpoint_mode='max', checkpoint_monitor='val_acc', dataloader_retry_limit=100, dataloader_retry_persistent_limit=5, dataloader_timeout=-1, default_hdfs_dir=None, default_root_dir=None, detect_anomaly=False, deterministic=False, enable_checkpoint=True, enable_qat=False, enable_speedmonitor=True, enable_trace=False, enable_versions=True, experiment_name=None, find_unused_parameters=True, grad_norm_layers=[], gradient_clip_val=None, limit_test_batches=None, limit_train_batches=None, limit_val_batches=None, log_every_n_steps=50, logger=True, max_epochs=100, max_steps=-1, optimizer_kwargs={'optimizer': {'type': 'torch.optim.AdamW', 'params': {'lr': 0.0001, 'betas': [0.9, 0.999], 'eps': 1e-06, 'weight_decay': 0.01, 'correct_bias': True, 'correct_bias_eps': False, 'bias_correction': True, 'adam_w_mode': True, 'amsgrad': False, 'set_grad_none': True, 'momentum': 0.0, 'nesterov': False}}, 'scheduler': {'type': 'cruise.optim.constant_lr', 'total_steps_param_name': 'total_iters', 'warmup_steps_param_name': 'warmup_iters', 'interval': 'epoch_end', 'params': {'warmup_step_rate': 0.0, 'start_factor': 0.3333333333333333, 'end_factor': 1e-07, 'num_cycles': 0.5, 'lr_end': 1e-07, 'power': 1.0}}}, precision=32, project_name=None, qat_kwargs={}, reload_dataloaders_every_n_epochs=-1, resume_ckpt_path=None, resume_loader_state=False, seed=None, strategy='ddp', summarize_model_depth=2, sync_batchnorm=False, sync_fit_metrics=None, val_check_interval=[1000, 1.0])}
***********

2023-03-03 15:17:43,058 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Adjusting root logger level to INFO

2023-03-03 15:17:43,161 INFO MainProcess set_nvidia_flags[/usr/local/lib/python3.7/dist-packages/cruise/trainer/accelerator/gpu.py:70]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

2023-03-03 15:17:43,161 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup validation every 1000 iteration steps

2023-03-03 15:17:43,161 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup validation every 1.0 epoch

2023-03-03 15:17:43,167 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Using version control root dir: "/mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_6", you can turn off this behavior by `enable_versions=false`

2023-03-03 15:17:43,167 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Setup ModelCheckpoint with local topk=10, hdfs topk=10

2023-03-03 15:17:43,199 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Full CLI saved to: /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_6/cruise_cli.yaml, /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_6/cruise_cli.json

2023-03-03 15:17:43,204 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:127]
obtain the local file `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a/config.yaml`

2023-03-03 15:17:43,205 INFO MainProcess cache_file[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:127]
obtain the local file `/root/.cache/easyguard/models/fashion_swin/0007f434eb731c6f5a799d7b773390dd6ab319bc97e314a17cac5254365b502a/pytorch_model.pth`

/usr/local/lib/python3.7/dist-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  ../aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
2023-03-03 15:17:45,208 INFO MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:497]
Pretrained: no missing_keys.

2023-03-03 15:17:45,209 WARNING MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:503]
=> Pretrained: unexpected_keys: 
+-----------------+
| unexpected_keys |
+-----------------+
| head.bias       |
| head.weight     |
+-----------------+


2023-03-03 15:17:45,209 INFO MainProcess load_pretrained_model_weights[/mnt/bn/multimodel-pretrain/codes/EasyGuard/easyguard/utils/auxiliary_utils.py:508]
=> Pretrained: load pretrained model with strict=False

[34m[1mwandb[0m: ‚≠êÔ∏è View project at [34m[4mhttps://ml.bytedance.net/experiment/tracking/detail?Id=project_20230116_29c3c128[0m
[34m[1mwandb[0m: üöÄ View run at [34m[4mhttps://ml.bytedance.net/experiment/tracking/detail?Id=project_20230116_29c3c128&selectedTrial=run_20230303_997e8559[0m
[34m[1mwandb[0m: wandb version 0.13.10 is available!  To upgrade, please run:
[34m[1mwandb[0m:  $ pip install wandb --upgrade
[34m[1mwandb[0m: Tracking run with wandb version 0.13.7
[34m[1mwandb[0m: Run data is saved locally in [35m[1m/mnt/bn/multimodel-pretrain/codes/EasyGuard/wandb/run-20230303_151754-1ei3yuyi[0m
[34m[1mwandb[0m: Run [1m`wandb offline`[0m to turn off syncing.
2023-03-03 15:17:57,023 INFO MainProcess _summarize_model[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2240]

  | Name              | Type              | Params
--------------------------------------------------------
0 | model             | FashionSwinModel  | 86.9 M
1 | model.patch_embed | PatchEmbed        | 6.5 K 
2 | model.pos_drop    | Dropout           | 0     
3 | model.layers      | ModuleList        | 86.7 M
4 | model.norm        | LayerNorm         | 2.0 K 
5 | model.avgpool     | AdaptiveAvgPool1d | 0     
6 | model.head        | Linear            | 205 K 
--------------------------------------------------------
86.9 M    Trainable params
0         Non-trainable params
86.9 M    Total params
347.793   Total estimated model params size (MB)

2023-03-03 15:17:57,024 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
===<PROGRESS: 0.00%, Epoch 0/100, 468 steps per epoch>===

2023-03-03 15:18:39,362 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[0->49]: Speed=75.59 sample/s, ETA=(0d:11h:13m), Time=(DATA:0.021|FW:0.332|BW:0.550|OPTIM:0.837), loss=5.30368, train_acc=0.00438

2023-03-03 15:19:17,514 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[50->99]: Speed=83.88 sample/s, ETA=(0d:10h:6m), Time=(DATA:0.000|FW:0.256|BW:0.475|OPTIM:0.762), loss=5.30509, train_acc=0.00375

2023-03-03 15:19:55,801 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[100->149]: Speed=83.58 sample/s, ETA=(0d:10h:7m), Time=(DATA:0.000|FW:0.257|BW:0.478|OPTIM:0.765), loss=5.30184, train_acc=0.00469

2023-03-03 15:20:34,198 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[150->199]: Speed=83.34 sample/s, ETA=(0d:10h:8m), Time=(DATA:0.000|FW:0.258|BW:0.480|OPTIM:0.767), loss=5.29792, train_acc=0.00656

2023-03-03 15:21:12,660 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[200->249]: Speed=83.20 sample/s, ETA=(0d:10h:8m), Time=(DATA:0.000|FW:0.259|BW:0.481|OPTIM:0.768), loss=5.29668, train_acc=0.00938

2023-03-03 15:21:51,337 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[250->299]: Speed=82.74 sample/s, ETA=(0d:10h:11m), Time=(DATA:0.000|FW:0.259|BW:0.481|OPTIM:0.773), loss=5.29169, train_acc=0.01031

2023-03-03 15:22:29,925 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[300->349]: Speed=82.93 sample/s, ETA=(0d:10h:9m), Time=(DATA:0.000|FW:0.260|BW:0.483|OPTIM:0.771), loss=5.28437, train_acc=0.01844

2023-03-03 15:23:08,569 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[350->399]: Speed=82.81 sample/s, ETA=(0d:10h:9m), Time=(DATA:0.000|FW:0.260|BW:0.483|OPTIM:0.772), loss=5.25781, train_acc=0.01969

2023-03-03 15:23:47,180 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[0] Batch[400->449]: Speed=82.88 sample/s, ETA=(0d:10h:8m), Time=(DATA:0.000|FW:0.260|BW:0.483|OPTIM:0.771), loss=5.07457, train_acc=0.04563

2023-03-03 15:24:02,546 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Summary of VAL Epoch[0] Global Step[468] Eval Batch[0->1]: val_acc=0.16753

2023-03-03 15:24:06,192 INFO MainProcess _log_info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2262]
[Rank 0] CruiseTrainer.save_checkpoint: Checkpoint saved to /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_6/checkpoints/epoch=0-step=468-val_acc=0.168.ckpt

2023-03-03 15:24:06,192 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
===<PROGRESS: 1.00%, Epoch 1/100, 468 steps per epoch>===

2023-03-03 15:24:32,120 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[468->499]: Speed=79.00 sample/s, ETA=(0d:10h:45m), Time=(DATA:0.025|FW:0.283|BW:0.506|OPTIM:0.796), loss=4.44946, train_acc=0.13867

2023-03-03 15:25:10,690 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[500->549]: Speed=82.97 sample/s, ETA=(0d:10h:6m), Time=(DATA:0.000|FW:0.259|BW:0.481|OPTIM:0.771), loss=3.96981, train_acc=0.20188

2023-03-03 15:25:49,301 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[550->599]: Speed=82.88 sample/s, ETA=(0d:10h:6m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.771), loss=3.57906, train_acc=0.27094

2023-03-03 15:26:27,925 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[600->649]: Speed=82.85 sample/s, ETA=(0d:10h:6m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.772), loss=3.36663, train_acc=0.30719

2023-03-03 15:27:06,525 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[650->699]: Speed=82.90 sample/s, ETA=(0d:10h:5m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.771), loss=3.16676, train_acc=0.33781

2023-03-03 15:27:45,129 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[700->749]: Speed=82.89 sample/s, ETA=(0d:10h:4m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.771), loss=2.99694, train_acc=0.36719

2023-03-03 15:28:23,715 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[750->799]: Speed=82.93 sample/s, ETA=(0d:10h:3m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.771), loss=2.81692, train_acc=0.40688

2023-03-03 15:29:02,378 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[800->849]: Speed=82.77 sample/s, ETA=(0d:10h:4m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.772), loss=2.84466, train_acc=0.40375

2023-03-03 15:29:40,956 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[1] Batch[850->899]: Speed=82.95 sample/s, ETA=(0d:10h:2m), Time=(DATA:0.000|FW:0.259|BW:0.482|OPTIM:0.771), loss=2.75046, train_acc=0.42156

2023-03-03 15:30:10,348 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
Summary of VAL Epoch[1] Global Step[936] Eval Batch[0->1]: val_acc=0.70660

2023-03-03 15:30:14,076 INFO MainProcess _log_info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/common_trainer.py:2262]
[Rank 0] CruiseTrainer.save_checkpoint: Checkpoint saved to /mnt/bn/multimodel-pretrain/codes/EasyGuard/cruise_logs/version_6/checkpoints/epoch=1-step=936-val_acc=0.707.ckpt

2023-03-03 15:30:14,077 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/utilities/rank_zero.py:51]
===<PROGRESS: 2.00%, Epoch 2/100, 468 steps per epoch>===

2023-03-03 15:30:26,004 INFO MainProcess _info[/usr/local/lib/python3.7/dist-packages/cruise/trainer/logger/console.py:194]
Epoch[2] Batch[936->949]: Speed=75.16 sample/s, ETA=(0d:11h:40m), Time=(DATA:0.048|FW:0.306|BW:0.528|OPTIM:0.817), loss=2.61695, train_acc=0.45982

